Group Relative Policy Optimization (GRPO) is a reinforcement learning technique that optimizes models by leveraging \textit{relative ranking} rather than relying on absolute correctness. Unlike traditional supervised learning methods, which use fixed labels and cross-entropy loss, GRPO dynamically updates model parameters based on comparisons between multiple generated responses. Recently, GRPO has been employed in \textit{DeepSeek-V2}, a groundbreaking work that has demonstrated its effectiveness in training large-scale language models. 

This work aims to explain GRPO using MNIST dataset as a simple classification task. By implementing GRPO in a neural network, we illustrate how multiple predictions per input can be \textit{ranked and optimized using policy gradients}, providing an intuitive and hands-on approach to understanding this optimization strategy. 
